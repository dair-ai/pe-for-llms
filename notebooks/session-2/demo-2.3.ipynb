{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Demo 2.4 - Data-Augmented Question Answering\n",
    "\n",
    "We are interested to build a personal learning assistant. The parts we need:\n",
    "\n",
    "- user question (input)\n",
    "- role prompting to mimic learning assistant role\n",
    "- relevant context obtained via knowledge source\n",
    "    - knowledge base/source (we are using lecture transcriptions for simplicity)\n",
    "- vector database to store the data source and support semantic search\n",
    "- response with source/citations (summarized output)\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dair-ai/pe-for-llms/blob/main/notebooks/session-2/demo-2.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data we want to use as source to augment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data source, we will use a transcription of Karpathy's recent lecture on GPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "with open('../data/kar-gpt.txt') as f:\n",
    "    text_data = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\" \")\n",
    "texts = text_splitter.split_text(text_data)\n",
    "\n",
    "# embeddings obtained from OpenAI (you can use open-source like FAISS)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the course about?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' This course is about understanding and appreciating how chat GPT works, and how to develop a transformer neural network. It requires proficiency in Python and some basic understanding of calculus and statistics.\\nSOURCES: 1, 7, 107, 108'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '\\nAnswer: This course is about training language models using Python and understanding the transformer neural network. It covers the basics of language modeling, such as multi-level perceptrons, and then focuses on the transformer neural network. It also covers fine tuning stages that can be used to perform tasks such as sentiment detection. SOURCES: 108, 1, 107, 7'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful personal assistant for learning.\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\n",
    "Given the summary above, help answer the following question from the user:\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create a prompt template\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "# query \n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Answer: This course is about understanding and appreciating how the transformer neural network works under the hood. It requires a proficiency in Python and some basic understanding of calculus and statistics. It also helps to have seen the Makemore series on the same YouTube channel, which introduces the language modeling framework. The course focuses on the transformer neural network, which was proposed in the 2017 paper \"Attention is All You Need\". SOURCES: 1, 2, 7, 21\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m The course focuses on the transformer neural network and requires a proficiency in Python and basic understanding of calculus and statistics.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': {'input_documents': [Document(page_content=\"And of course this can copy paste to any arbitrary text dataset that you like. But my goal really here is to just make you understand and appreciate how under the hood chat GPT works. And really all that's required is a proficiency in Python and some basic understanding of calculus and statistics. And it would help if you also see my previous videos on the same YouTube channel in particular my Makemore series where I define smaller and simpler neural network language models. So multi-level perceptrons and so on. It really introduces the language modeling framework. And then here in this video we're going to focus on the transformer neural network itself. Okay so I created a new Google colab Jupyter notebook here. And this will allow me to later easily share this code that we're going to develop together with you so you can follow along. So this will be in the video description now here I've just done some preliminaries. I downloaded the dataset, the tiny Shakespeare dataset at the URL\", metadata={'source': '7'}),\n",
       "   Document(page_content=\"example of the prompt. People have come up with many, many examples and there are entire websites that index interactions with chat GPT and so many of them are quite humorous. Explain HTML to me like I'm a dog. Write reliefs notes for chest two. Write a note about Elon Musk buying a Twitter and so on. So as an example, please write a breaking news article about a leaf falling from a tree. In a shocking turn of events, a leaf has fallen from a tree in the local park. Witnesses report that the leaf which was previously attached to a branch of a tree, detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty remarkable system and it is what we call a language model because it models the sequence of words or characters or tokens more generally and it knows how sort of words follow each other in English language. And so from its perspective, what it is doing is it is completing the sequence. So I give it the start of a sequence and it completes the sequence\", metadata={'source': '1'}),\n",
       "   Document(page_content=\"with the outcome and so it's a language model in that sense. Now I would like to focus on the under the hood of under the hood components of what makes chat GPT work. So what is the neural network under the hood that models the sequence of these words and that comes from this paper called attention is all you need in 2017 a landmark paper a landmark paper in AI that produced and proposed the transformer architecture. So GPT is short for generally, generatively pre-trained transformer. So transformer is the neural network that actually does all the handy lifting under the hood. It comes from this paper in 2017. Now if you read this paper, this reads like a pretty random machine translation paper and that's because I think the authors didn't fully anticipate the impact that the transformer would have on the field and this architecture that they produced in the context of machine translation in their case actually ended up taking over the rest of AI in the next five years after. And so\", metadata={'source': '2'}),\n",
       "   Document(page_content=\"transformer is going to simultaneously process all these examples and then look up the correct integers to predict in every one of these positions in the tensor y okay so now that we have our batch of input that we'd like to feed into a transformer let's start basically feeding this into neural networks now we're going to start off with the simplest possible neural network which in the case of language modeling in my opinion is the bi-gram language model and we've covered the bi-gram language model in my make more series in a lot of depth and so here I'm going to sort of go faster and let's just implement the PyTorch module directly that implements the bi-gram language model so I'm importing the PyTorch and NModule for reproducibility and then here I'm constructing a bi-gram language model which is a subclass of NModule and then I'm calling it and I'm passing at the inputs and the targets and I'm just printing now when the inputs and targets come here you see that I'm just taking the\", metadata={'source': '21'})],\n",
       "  'question': 'What is the course about?'},\n",
       " 'output': ' The course focuses on the transformer neural network and requires a proficiency in Python and basic understanding of calculus and statistics.'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "agent_response = PromptTemplate(\n",
    "    input_variables=[\"response\"],\n",
    "    template=\"Answer as the personal assistant. Just take the answer from the previous response {response}. \\nAgent:\"\n",
    ")\n",
    "\n",
    "agent_chain = LLMChain(llm=llm, prompt=agent_response)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, agent_chain], verbose=True)\n",
    "\n",
    "query = \"What is the course about?\"\n",
    "overall_chain({\"input\": {\"input_documents\": docs ,\"question\": query}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
