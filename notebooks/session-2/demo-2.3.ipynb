{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Demo 2.4 - Data-Augmented Question Answering\n",
    "\n",
    "We are interested to build a personal learning assistant. The parts we need:\n",
    "\n",
    "- user question (input)\n",
    "- role prompting to mimic learning assistant role\n",
    "- relevant context obtained via knowledge source\n",
    "    - knowledge base/source (we are using lecture transcriptions for simplicity)\n",
    "- vector database to store the data source and support semantic search\n",
    "- personalized response with source/citations (summarized output)\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dair-ai/pe-for-llms/blob/main/notebooks/session-2/demo-2.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data we want to use as source to augment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data source, we will use a transcription of Karpathy's recent lecture on GPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "with open('../data/kar-gpt.txt') as f:\n",
    "    text_data = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\" \")\n",
    "texts = text_splitter.split_text(text_data)\n",
    "\n",
    "# embeddings obtained from OpenAI (you can use open-source like FAISS)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the course about?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' This course is about understanding and appreciating how chat GPT works, and how to develop a transformer neural network. It requires proficiency in Python and some basic understanding of calculus and statistics.\\nSOURCES: 1, 7, 107, 108'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '\\nAnswer: This course is about training language models using Python and understanding the transformer neural network. It covers the basics of language modeling, such as multi-level perceptrons, and then focuses on the transformer neural network. It also covers fine tuning stages that can be used to perform tasks such as sentiment detection. SOURCES: 108, 1, 107, 7'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "\n",
    "Given the summary above, help answer the following question from the user:\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# create a prompt template\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "# query \n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Answer: This course is about understanding and appreciating how the transformer neural network works under the hood. It requires a proficiency in Python and some basic understanding of calculus and statistics. It also helps to have seen the Makemore series on the same YouTube channel, which introduces the language modeling framework. The course focuses on the transformer neural network, which is a language model that models the sequence of words or characters or tokens more generally. SOURCES: 1, 2, 7, 21.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "The course focuses on the transformer neural network, which is a language model that models the sequence of words or characters or tokens more generally.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': '\\nThe course focuses on the transformer neural network, which is a language model that models the sequence of words or characters or tokens more generally.'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "agent_response = PromptTemplate(\n",
    "    input_variables=[\"response\"],\n",
    "    template=\"\"\"You are a personal learning assistant. \n",
    "    Just take the answer from the previous response {response} and output it as the final response.\n",
    "\n",
    "    Agent:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "agent_chain = LLMChain(llm=llm, prompt=agent_response)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, agent_chain], verbose=True)\n",
    "\n",
    "query = \"What is the course about?\"\n",
    "overall_chain({\"input\": {\"input_documents\": docs ,\"question\": query}}, return_only_outputs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Add another chain that connects with the previous `agent_chain` to create another agent that tries to be helpful and follows up with a question if it helps to keep the conversation going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
