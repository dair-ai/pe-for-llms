{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Demo 2.4 - Data-Augmented Question Answering\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dair-ai/pe-for-llms/blob/main/notebooks/session-2/demo-2.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the data we want to use as source to augment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data source, we will use a transcription of Karpathy's recent lecture on GPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/state_of_the_union.txt') as f:\n",
    "with open('../data/kar-gpt.txt') as f:\n",
    "    text_data = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\" \")\n",
    "texts = text_splitter.split_text(text_data)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the course about?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' This course is about understanding and appreciating how chat GPT works, and how to develop a transformer neural network. It requires proficiency in Python and some basic understanding of calculus and statistics.\\nSOURCES: 1, 7, 107, 108'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content=\"of fine tuning which we did not cover and that could be simple supervised fine tuning or it can be something more fancy like we see in charge of GPT we actually train a reward model and then do rounds of PPO to align it with respect to the reward model so there's a lot more that can be down on top of it I think for now we're starting to get to about two hours mark so I'm going to kind of finish here I hope you enjoyed the lecture and yeah go forth and transform see you later\", metadata={'source': '108'}),\n",
       "  Document(page_content=\"example of the prompt. People have come up with many, many examples and there are entire websites that index interactions with chat GPT and so many of them are quite humorous. Explain HTML to me like I'm a dog. Write reliefs notes for chest two. Write a note about Elon Musk buying a Twitter and so on. So as an example, please write a breaking news article about a leaf falling from a tree. In a shocking turn of events, a leaf has fallen from a tree in the local park. Witnesses report that the leaf which was previously attached to a branch of a tree, detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty remarkable system and it is what we call a language model because it models the sequence of words or characters or tokens more generally and it knows how sort of words follow each other in English language. And so from its perspective, what it is doing is it is completing the sequence. So I give it the start of a sequence and it completes the sequence\", metadata={'source': '1'}),\n",
       "  Document(page_content=\"on a tiny Shakespeare and got sensible results all of the training code is roughly 200 lines of code I will be releasing this code base so also it comes with all the get log commits along the way as we built it up in addition to this code I'm going to release the notebook of course the Google Cloud and I hope that gave you a sense for how you can train these models like say GPT3 that will be architecturally basically identical to what we have but they are somewhere between 10,000 and 1 million times bigger depending on how you count and so that's all I have for now we did not talk about any of the fine tuning stages that would typically go on top of this so if you're interested in something that's not just language modeling but you actually want to you know say perform tasks or you want them to be aligned in a specific way or you want to detect sentiment or anything like that basically anytime you don't want something that's just a document completer you have to complete further stages\", metadata={'source': '107'}),\n",
       "  Document(page_content=\"And of course this can copy paste to any arbitrary text dataset that you like. But my goal really here is to just make you understand and appreciate how under the hood chat GPT works. And really all that's required is a proficiency in Python and some basic understanding of calculus and statistics. And it would help if you also see my previous videos on the same YouTube channel in particular my Makemore series where I define smaller and simpler neural network language models. So multi-level perceptrons and so on. It really introduces the language modeling framework. And then here in this video we're going to focus on the transformer neural network itself. Okay so I created a new Google colab Jupyter notebook here. And this will allow me to later easily share this code that we're going to develop together with you so you can follow along. So this will be in the video description now here I've just done some preliminaries. I downloaded the dataset, the tiny Shakespeare dataset at the URL\", metadata={'source': '7'})],\n",
       " 'question': 'What is the course about?',\n",
       " 'output_text': '\\nEl curso trata sobre cómo entrenar un modelo de lenguaje usando una red neuronal de transformadores. Se requiere una proficiencia en Python y una comprensión básica de cálculo y estadística. El curso también cubre el ajuste fino, que puede ser un ajuste supervisado simple o algo más sofisticado como el entrenamiento de un modelo de recompensa y luego varias rondas de PPO para alinearlo con el modelo de recompensa.\\n\\nFUENTES:\\n108, 1, 107, 7'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "ALWAYS return a \"SOURCES\" part in your answer.\n",
    "Respond in Spanish.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{summaries}\n",
    "=========\n",
    "FINAL ANSWER IN SPANISH:\"\"\"\n",
    "\n",
    "# create a prompt template\n",
    "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "# query \n",
    "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
    "query = \"What is the course about?\"\n",
    "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about chain types here: https://docs.langchain.com/docs/components/chains/index_related_chains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
