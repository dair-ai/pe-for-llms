{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Demo 2.2 - Model Reliability & Enhancing LLMs\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/dair-ai/pe-for-llms/blob/main/notebooks/session-2/demo-2.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `text-davinci-003` for all examples in this tutorial. You can easily switch to `gpt-3.5-turbo`, however, note that this model is more expensive and slower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm instance\n",
    "llm_text_davinci = OpenAI(model_name='text-davinci-003', temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting token usage\n",
    "\n",
    "We will use `tiktoken`, an open-source tokenizer by OpenAI.\n",
    "\n",
    "https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load encoding by name\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# load the correct encoding by passing the model name\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1097, 8430, 6380, 3432]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize text\n",
    "encoding.encode(\"I am feeling happy today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count tokens\n",
    "len(encoding.encode(\"I am feeling happy today\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gpt-4 model\n",
    "gpt4_encoding = tiktoken.encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 1097, 8430, 6380, 3432]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt4_encoding.encode(\"I am feeling happy today\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate token usage with LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 264\n",
      "\tPrompt Tokens: 8\n",
      "\tCompletion Tokens: 256\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.005280000000000001\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# anything inside the context manager will be tracked\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a short story about a robot\")\n",
    "    print(cb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be clear and specific when prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, couldn't find a movie to recommend today.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Your task is to recommend movies to a customer. \n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}. \n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "If you don't have a movie to recommend, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\n",
    "Customer: Please recommend a movie based on my interests.\n",
    "Agent: \n",
    "\"\"\"\n",
    "\n",
    "llm_text_davinci(prompt.format(global_trending_movies=global_trending_movies))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example where the customer provides information about interests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I recommend Spider-Man: No Way Home. It's a highly anticipated sequel to the Marvel Cinematic Universe and is sure to be a hit with fans of super-hero movies.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Your task is to recommends movies to a customer. \n",
    "\n",
    "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}. \n",
    "\n",
    "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
    "\n",
    "If you don't have a movie to recommend, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
    "\n",
    "Customer: I love super-hero movies. Please recommend a movie based on my interests.\n",
    "Agent: \n",
    "\"\"\"\n",
    "\n",
    "llm_text_davinci(prompt.format(global_trending_movies=global_trending_movies))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Delimiters to Distinguish Components of a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Convert the following code block in the ### <code> ### section to Python:\n",
    "\n",
    "###\n",
    "strings2 = Array[]\n",
    "strings2.push(\"one\")\n",
    "strings2.push(\"two\")\n",
    "strings2.push(\"THREE\")\n",
    "### \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "strings2 = []\n",
       "strings2.append(\"one\")\n",
       "strings2.append(\"two\")\n",
       "strings2.append(\"THREE\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(\"```python\" + llm_text_davinci(prompt) + \"\\n```\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "{\n",
       "  \"product_name\": \"Nike Air Max 270 React\",\n",
       "  \"product_brand\": \"Nike\"\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
    "\n",
    "Product Description: Introducing the Nike Air Max 270 React: a comfortable and stylish sneaker that combines two of Nike's best technologies. With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
    "\n",
    "###\n",
    "product_name: the name of the product\n",
    "product_bran: the name of the brand (if any) \n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "IPython.display.Markdown(llm_text_davinci(prompt))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Length of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We apologize for the inconvenience caused by the shipping issue with your recent order. We have located your package and it is on its way to you. We have taken steps to prevent similar issues from occurring in the future. Thank you for your understanding.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is: given a customer support email, which is delimited with ###, generate a shorter 1-2 sentence response.\n",
    "\n",
    "###\n",
    "Dear [Customer],\n",
    "\n",
    "We hope this email finds you well. We wanted to update you on the shipping issue you experienced with your recent order. After investigating the issue, we have located your package and it is currently on its way to you. We apologize for any inconvenience this may have caused and thank you for your patience and understanding while we resolved this matter.\n",
    "\n",
    "Please note that we have taken steps to prevent similar issues from occurring in the future. We have improved our shipping tracking system and are now better equipped to ensure that packages arrive on time and in good condition. We take the quality of our service very seriously and want to ensure that all of our customers have a positive experience when shopping with us.\n",
    "\n",
    "Once again, we apologize for any inconvenience this may have caused and hope that you will continue to shop with us in the future. If you have any further questions or concerns, please do not hesitate to contact us. We are always here to help and ensure that your shopping experience is a positive one.\n",
    "\n",
    "Thank you for your understanding.\n",
    "\n",
    "Best regards,\n",
    "\n",
    "[Your Name]\n",
    "\n",
    "Customer Support Team\n",
    "###\n",
    "\"\"\"\n",
    "\n",
    "llm_text_davinci(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid deviating and constraining the output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it helps to be more specific about what output you expect to avoid the model deviating from the main task of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThe Grand Budapest Hotel (2014). This Wes Anderson classic is a delightful comedy-drama set in a European hotel in the 1930s. It follows the adventures of the hotel's concierge, Gustave H, and his loyal lobby boy, Zero Moustafa, as they try to solve a mystery involving a priceless painting. With an all-star cast, including Ralph Fiennes, Adrien Brody, and Willem Dafoe, this is a must-see for any movie fan.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_text_davinci(\"Recommend a movie for Saturday:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe Princess Bride'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_text_davinci(\"Recommend a movie for Saturday. Just say the movie, no need for explanations!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Task into Subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = \"\"\"\n",
    "Summer Beats Festival\n",
    "\n",
    "The event will be held at the beautiful seaside location of Ocean Park in Miami, Florida.\n",
    "\n",
    "The festival will take place over two days, from July 15th to July 16th.\n",
    "\n",
    "The Summer Beats Festival will feature a fantastic lineup of popular musical artists and bands from a variety of genres. Attendees can expect to dance and sing along to live performances from headliners such as Taylor Swift, Bruno Mars, and Post Malone. In addition to the main stage, there will be several smaller stages scattered throughout the park featuring up-and-coming artists and DJs.\n",
    "\n",
    "The festival will also offer a wide variety of food and drink options for attendees to enjoy. From classic festival fare like hot dogs and funnel cakes to more gourmet offerings like sushi and craft beer, there will be something to suit every taste.\n",
    "\n",
    "Families with children are welcome, and there will be plenty of activities to keep the little ones entertained. The festival will offer a dedicated children's area with carnival games, face painting, and other fun activities.\n",
    "\n",
    "For those looking for a more luxurious experience, the Summer Beats Festival will also offer a VIP area with premium viewing of the main stage, private bars, and lounges, and other exclusive perks.\n",
    "\n",
    "Overall, the Summer Beats Festival promises to be an unforgettable event for music lovers of all ages. With a stunning location, a great lineup of artists, and plenty of activities and amenities, it's sure to be the highlight of the summer!\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Your task is to extract the date of the event and the name of the event. The event is delimited by ### ###.\n",
    "\n",
    "###\n",
    "Event: {event}\n",
    "###\n",
    "\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Event: Summer Beats Festival\\nDate: July 15th to July 16th'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm(prompt.format(event=event))\n",
    "llm_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you add more tasks within the prompt ensure you be more detailed and specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Summer Beats Festival will feature a fantastic lineup of musical artists and offer a wide variety of food and drink options, as well as activities for families with children. It will take place from 07/15-07/16.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is to explain the event in 2 sentences. Extract the date of the event and the name of the event. The event is delimited by ### ###. \n",
    "\n",
    "Transform the dates into a MM/DD.\n",
    "\n",
    "###\n",
    "Event: {event}\n",
    "###\n",
    "\n",
    "Output format: Explanation | event name | date\n",
    "\"\"\"\n",
    "llm_out = llm(prompt.format(event=event))\n",
    "llm_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program-aided Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm instance\n",
    "pal_llm = OpenAI(model_name='text-davinci-003', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "PENGUIN_PROMPT = '''\n",
    "\"\"\"\n",
    "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
    "name, age, height (cm), weight (kg) \n",
    "Louis, 7, 50, 11\n",
    "Bernard, 5, 80, 13\n",
    "Vincent, 9, 60, 11\n",
    "Gwen, 8, 70, 15\n",
    "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. \n",
    "We now add a penguin to the table:\n",
    "James, 12, 90, 12\n",
    "How many penguins are less than 8 years old?\n",
    "\"\"\"\n",
    "# Put the penguins into a list.\n",
    "penguins = []\n",
    "penguins.append(('Louis', 7, 50, 11))\n",
    "penguins.append(('Bernard', 5, 80, 13))\n",
    "penguins.append(('Vincent', 9, 60, 11))\n",
    "penguins.append(('Gwen', 8, 70, 15))\n",
    "# Add penguin James.\n",
    "penguins.append(('James', 12, 90, 12))\n",
    "# Find penguins under 8 years old.\n",
    "penguins_under_8_years_old = [penguin for penguin in penguins if penguin[1] < 8]\n",
    "# Count number of penguins under 8.\n",
    "num_penguin_under_8 = len(penguins_under_8_years_old)\n",
    "answer = num_penguin_under_8\n",
    "\"\"\"\n",
    "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
    "name, age, height (cm), weight (kg) \n",
    "Louis, 7, 50, 11\n",
    "Bernard, 5, 80, 13\n",
    "Vincent, 9, 60, 11\n",
    "Gwen, 8, 70, 15\n",
    "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
    "Which is the youngest penguin?\n",
    "\"\"\"\n",
    "# Put the penguins into a list.\n",
    "penguins = []\n",
    "penguins.append(('Louis', 7, 50, 11))\n",
    "penguins.append(('Bernard', 5, 80, 13))\n",
    "penguins.append(('Vincent', 9, 60, 11))\n",
    "penguins.append(('Gwen', 8, 70, 15))\n",
    "# Sort the penguins by age.\n",
    "penguins = sorted(penguins, key=lambda x: x[1])\n",
    "# Get the youngest penguin's name.\n",
    "youngest_penguin_name = penguins[0][0]\n",
    "answer = youngest_penguin_name\n",
    "\"\"\"\n",
    "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
    "name, age, height (cm), weight (kg) \n",
    "Louis, 7, 50, 11\n",
    "Bernard, 5, 80, 13\n",
    "Vincent, 9, 60, 11\n",
    "Gwen, 8, 70, 15\n",
    "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
    "What is the name of the second penguin sorted by alphabetic order?\n",
    "\"\"\"\n",
    "# Put the penguins into a list.\n",
    "penguins = []\n",
    "penguins.append(('Louis', 7, 50, 11))\n",
    "penguins.append(('Bernard', 5, 80, 13))\n",
    "penguins.append(('Vincent', 9, 60, 11))\n",
    "penguins.append(('Gwen', 8, 70, 15))\n",
    "# Sort penguins by alphabetic order.\n",
    "penguins_alphabetic = sorted(penguins, key=lambda x: x[0])\n",
    "# Get the second penguin sorted by alphabetic order.\n",
    "second_penguin_name = penguins_alphabetic[1][0]\n",
    "answer = second_penguin_name\n",
    "\"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "'''.strip() + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which is the oldest penguin?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Put the penguins into a list.\n",
      "penguins = []\n",
      "penguins.append(('Louis', 7, 50, 11))\n",
      "penguins.append(('Bernard', 5, 80, 13))\n",
      "penguins.append(('Vincent', 9, 60, 11))\n",
      "penguins.append(('Gwen', 8, 70, 15))\n",
      "# Sort the penguins by age.\n",
      "penguins = sorted(penguins, key=lambda x: x[1], reverse=True)\n",
      "# Get the oldest penguin's name.\n",
      "oldest_penguin_name = penguins[0][0]\n",
      "answer = oldest_penguin_name\n"
     ]
    }
   ],
   "source": [
    "llm_out = pal_llm(PENGUIN_PROMPT.format(question=question))\n",
    "print(llm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vincent\n"
     ]
    }
   ],
   "source": [
    "exec(llm_out)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct\n",
    "\n",
    "Example of what the ReAct prompting approach looks like visually: https://www.comet.com/examples/langchain/view/Tgq750ZDhX17skjsgBCcs2w2z/panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Olivia Wilde's boyfriend is Jason Sudeikis. His current age is 43, so raised to the 0.23 power, his age is approximately 4.9."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"\"\"\n",
    "\n",
    "response = pal_llm(prompt)\n",
    "\n",
    "IPython.display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.\n",
      "Action: Search\n",
      "Action Input: \"Olivia Wilde boyfriend\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out Harry Styles' age.\n",
      "Action: Search\n",
      "Action Input: \"Harry Styles age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m29 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate 29 raised to the 0.23 power.\n",
      "Action: Calculator\n",
      "Action Input: 29^0.23\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.169459462491557\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the agent\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out which teams are in the 2023 NBA western conference finals\n",
      "Action: Search\n",
      "Action Input: \"2023 NBA western conference finals teams\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe Denver Nuggets take on the Los Angeles Lakers as the 2023 NBA Western Conference finals begin on Tuesday.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The Denver Nuggets and the Los Angeles Lakers.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Denver Nuggets and the Los Angeles Lakers.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Which teams made it into the 2023 NBA western conference finals?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
