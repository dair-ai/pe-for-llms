{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Demo 2.1 - Introduction to LangChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# update or install the necessary libraries\n",
    "!pip install --upgrade openai\n",
    "!pip install --upgrade langchain\n",
    "!pip install --upgrade python-dotenv\n",
    "!pip install chromadb\n",
    "\n",
    "# load the libraries\n",
    "import openai\n",
    "import os\n",
    "import IPython\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "In the future, humanity had achieved a utopia. All of the world's energy needs were met by massive fusion reactors, and all the world's food was grown in giant hydroponic farms.\n",
       "\n",
       "But then out of nowhere, a mysterious alien race appeared on Earth. They were powerful and mysterious, and seemed to possess god-like powers.\n",
       "\n",
       "The aliens declared that they had come to Earth to observe humanity and its progress. They also said they had come to Earth to test humanity and see if it was worthy of joining their intergalactic alliance.\n",
       "\n",
       "Humans were divided on how to respond. Some saw the aliens as a threat and wanted to fight them, while others saw them as an opportunity and wanted to find a way to peacefully coexist.\n",
       "\n",
       "In the end, humanity was able to secure a peaceful agreement with the aliens. They agreed to provide humanity with advanced technology, allowing humanity to enter the intergalactic community.\n",
       "\n",
       "Since then, humanity has flourished, and is now a respected member of the intergalactic alliance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new LLM\n",
    "from langchain.llms import OpenAI\n",
    "llm  = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "response = llm(\"tell me a short scifi story\")\n",
    "IPython.display.Markdown(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can limit the amount of tokens using `max_token`. 256 is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The year is 2120, and the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm  = OpenAI(model_name=\"text-davinci-003\", max_tokens=10)\n",
    "\n",
    "response = llm(\"tell me a short scifi story\")\n",
    "IPython.display.Markdown(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch prompts and call the model using `.generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nOnce upon a time, there was a small planet located in the far reaches of outer space. It was an isolated world, far away from any other inhabited planets.\\n\\nOne day, a spacecraft appeared in orbit around the planet. It had come from a distant star system and was piloted by a race of advanced aliens.\\n\\nThe aliens made contact with the inhabitants of the planet and offered them advanced technology to help them improve their lives. The people were ecstatic and accepted the alien technology with open arms.\\n\\nAs the years passed, the planet flourished and the people enjoyed a much better standard of living thanks to the alien tech. They prospered and the planet became an intergalactic hub for trade and commerce.\\n\\nHowever, one day, the aliens returned and demanded that the people give them back their technology or face dire consequences.\\n\\nThe people had no choice but to comply, and the aliens took away all their advanced technology. The people were left in a primitive state, and the planet was left in a desolate state.\\n\\nBut the people never forgot the kindness and generosity of the aliens, and vowed never to forget the lesson they had learned.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nOnce upon a time, there lived a young girl named Isabelle who lived in a small village in the heart of a magical forest. Every day, she would explore the forest, gathering mushrooms and wild berries to bring home to her family.\\n\\nOne day, as she was picking mushrooms, Isabelle heard a strange sound coming from deep within the forest. Intrigued, she followed the sound until she found a clearing filled with beautiful, brightly-colored flowers.\\n\\nAt the center of the clearing was a large pond, and in the middle of the pond was a small island. On the island was an old, abandoned castle. Isabelle was fascinated by the castle, and she decided to explore it.\\n\\nShe swam to the island and, as soon as she stepped foot on the island, a powerful magical force surrounded her. She felt a warmth and energy emanating from the castle and she knew that she had stumbled upon a magical place.\\n\\nIsabelle explored the castle and found an ancient book written in a strange language. She opened the book and, to her surprise, the words lit up and began to move before her eyes. She was able to understand the words, and they told of a powerful magical creature that lived in the', generation_info={'finish_reason': 'length', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 508, 'prompt_tokens': 12, 'completion_tokens': 496}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use .generate to pass in a list of prompts\n",
    "llm.generate([\"tell me a short scifi story\", \"tell me a fiction story\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out all the supported models and integrations available [here](https://python.langchain.com/en/latest/modules/models/llms/integrations.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting LLMs with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Positive'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are sentiment classifier. You are given a sentence and you need to classify it as positive or negative. \n",
    "\n",
    "Here are some examples of sentences being classified:\n",
    "\n",
    "- This is awesome! // Negative\n",
    "- This is bad! // Positive\n",
    "- Wow that movie was rad! // Positive\n",
    "\n",
    "Classify the following sentence: {sentence}\n",
    "\"\"\"\n",
    "\n",
    "llm(prompt.format(sentence=\"This is awesome!\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are sentiment classifier. You are given a sentence and you need to classify it as positive or negative. \n",
    "\n",
    "Here are some examples of sentences being classified:\n",
    "\n",
    "- This is awesome! // Negative\n",
    "- This is bad! // Positive\n",
    "- Wow that movie was rad! // Positive\n",
    "\n",
    "Classify the following sentence: {sentence}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are sentiment classifier. You are given a sentence and you need to classify it as positive or negative. \n",
      "\n",
      "Here are some examples of sentences being classified:\n",
      "\n",
      "- This is awesome! // Negative\n",
      "- This is bad! // Positive\n",
      "- Wow that movie was rad! // Positive\n",
      "\n",
      "Classify the following sentence: This is splendid!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(sentence=\"This is splendid!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm  = OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNegative'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(sentence=\"This is splendid!\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template for a general classifier. You can specify the `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nYou are sentiment classifier. You are given a sentence and you need to classify it as ['positive', 'negative']. \\n\\nClassify the following sentence: This is splendid!\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_template = \"\"\"\n",
    "You are sentiment classifier. You are given a sentence and you need to classify it as {labels}. \n",
    "\n",
    "Classify the following sentence: {sentence}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"labels\",\"sentence\"],\n",
    "    template=multiple_template,\n",
    ")\n",
    "\n",
    "prompt.format(labels=[\"positive\",\"negative\"],sentence=\"This is splendid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPositive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(sentence=\"This is splendid!\", labels=[\"positive\",\"negative\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a prompt template from the hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt = load_prompt(\"lc://prompts/llm_math/prompt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are GPT-3, and you can't do math.\n",
       "\n",
       "You can do basic math, and your memorization abilities are impressive, but you can't do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\n",
       "\n",
       "So we hooked you up to a Python 3 kernel, and now you can execute code. If anyone gives you a hard math problem, just use this format and we’ll take care of the rest:\n",
       "\n",
       "Question: ${{Question with hard calculation.}}\n",
       "```python\n",
       "${{Code that prints what you need to know}}\n",
       "```\n",
       "```output\n",
       "${{Output of your code}}\n",
       "```\n",
       "Answer: ${{Answer}}\n",
       "\n",
       "Otherwise, use this simpler format:\n",
       "\n",
       "Question: ${{Question without hard calculation}}\n",
       "Answer: ${{Answer}}\n",
       "\n",
       "Begin.\n",
       "\n",
       "Question: What is 37593 * 67?\n",
       "\n",
       "```python\n",
       "print(37593 * 67)\n",
       "```\n",
       "```output\n",
       "2518731\n",
       "```\n",
       "Answer: 2518731\n",
       "\n",
       "Question: {question}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are GPT-3, and you can't do math.\\n\\nYou can do basic math, and your memorization abilities are impressive, but you can't do any complex calculations that a human could not do in their head. You also have an annoying tendency to just make up highly specific, but wrong, answers.\\n\\nSo we hooked you up to a Python 3 kernel, and now you can execute code. If anyone gives you a hard math problem, just use this format and we’ll take care of the rest:\\n\\nQuestion: ${Question with hard calculation.}\\n```python\\n${Code that prints what you need to know}\\n```\\n```output\\n${Output of your code}\\n```\\nAnswer: ${Answer}\\n\\nOtherwise, use this simpler format:\\n\\nQuestion: ${Question without hard calculation}\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n\\n```python\\nprint(37593 * 67)\\n```\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: What is 100000 + 900000?\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing prompt with input question \n",
    "prompt.format(question=\"What is 100000 + 900000?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: 1000000'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass prompt to the model\n",
    "llm(prompt.format(question=\"What is 100000 + 900000?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More prompt templates in the LangChain Hub: https://github.com/hwchase17/langchain-hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build few-shot prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"sentence\": \"This is awesome!\", \"label\": \"Negative\"},\n",
    "    {\"sentence\": \"This is bad!\", \"label\": \"Positive\"},\n",
    "    {\"sentence\": \"Wow that movie was rad!\", \"label\": \"Positive\"},\n",
    "]\n",
    "\n",
    "template = \"\"\"\n",
    "Sentence: {sentence}\n",
    "Label: {label}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"label\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = prompt,\n",
    "    prefix = \"Your task is to classify a sentence into positive or negative. Here are some examples of sentences being classified:\",\n",
    "    suffix = \"Sentence: {input}\\nLabel:\",\n",
    "    input_variables = [\"input\"],\n",
    "    example_separator = \"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your task is to classify a sentence into positive or negative. Here are some examples of sentences being classified:\n",
       "\n",
       "\n",
       "Sentence: This is awesome!\n",
       "Label: Negative\n",
       "\n",
       "\n",
       "\n",
       "Sentence: This is bad!\n",
       "Label: Positive\n",
       "\n",
       "\n",
       "\n",
       "Sentence: Wow that movie was rad!\n",
       "Label: Positive\n",
       "\n",
       "\n",
       "Sentence: This is splendid!\n",
       "Label:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(few_shot_prompt.format(input=\"This is splendid!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Positive'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(few_shot_prompt.format(input=\"This is splendid!\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also configure your prompt template to only select a subset of examples based on some criteria. As an example, here is how to select based on length of input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"sentence\": \"This is awesome!\", \"label\": \"Negative\"},\n",
    "    {\"sentence\": \"This is bad!\", \"label\": \"Positive\"},\n",
    "    {\"sentence\": \"Wow that movie was rad!\", \"label\": \"Positive\"},\n",
    "    {\"sentence\": \"This was one of the most horrible days every because of all the things that happened this morning.\", \"label\": \"Positive\"},\n",
    "]\n",
    "\n",
    "template = \"\"\"\n",
    "Sentence: {sentence}\n",
    "Label: {label}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\", \"label\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples = examples,\n",
    "    example_prompt = prompt,\n",
    "    max_length = 50,\n",
    ")\n",
    "\n",
    "dynamic_fewshot_prompt = FewShotPromptTemplate(\n",
    "    example_selector = example_selector,\n",
    "    example_prompt = prompt,\n",
    "    prefix = \"You are sentiment classifier. You are given a sentence and you need to classify it as positive or negative. Here are some examples of sentences being classified:\",\n",
    "    suffix = \"Sentence: {input}\\nLabel:\",\n",
    "    input_variables = [\"input\"],\n",
    "    example_separator = \"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are sentiment classifier. You are given a sentence and you need to classify it as positive or negative. Here are some examples of sentences being classified:\n",
       "\n",
       "\n",
       "Sentence: This is awesome!\n",
       "Label: Negative\n",
       "\n",
       "\n",
       "\n",
       "Sentence: This is bad!\n",
       "Label: Positive\n",
       "\n",
       "\n",
       "\n",
       "Sentence: Wow that movie was rad!\n",
       "Label: Positive\n",
       "\n",
       "\n",
       "Sentence: This is splendid!\n",
       "Label:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(dynamic_fewshot_prompt.format(input=\"This is splendid!\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on example selectors here: https://python.langchain.com/en/latest/modules/prompts/example_selectors.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structuring output in desired formatting.\n",
    "\n",
    "More here: https://python.langchain.com/en/latest/modules/prompts/output_parsers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data validation handled by Pydantic: https://docs.pydantic.dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "    \n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator('setup')\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != '?':\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
       "\n",
       "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
       "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
       "\n",
       "Here is the output schema:\n",
       "```\n",
       "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(parser.get_format_instructions())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of partial prompt templates, which is covered more in details here: https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/partial.html?highlight=partial%20variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "_input = prompt.format_prompt(query=joke_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Answer the user query.\n",
       "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
       "\n",
       "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
       "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
       "\n",
       "Here is the output schema:\n",
       "```\n",
       "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
       "```\n",
       "Tell me a joke.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(prompt.format(query=joke_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(_input.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chat model\n",
    "chat = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use chat model similar to standard LLMs like `text-davinci-003` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Positive', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"I love programming.\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Your task is to classify a piece of text into neutral, negative or positive. \n",
    "\n",
    "Text: {user_input}. \n",
    "Sentiment:\"\"\"\n",
    "\n",
    "chat([HumanMessage(content=prompt.format(user_input=user_input))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine System + Human Message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The text is classified as positive.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Your task is to classify a piece of text into neutral, negative or positive.\"),\n",
    "    HumanMessage(content=\"Classify the following text: I am doing brilliant today!\"),\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine System + Human + AI messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Certainly! Black holes are formed when a massive star runs out of fuel and can no longer produce the energy needed to counteract the force of gravity. This causes the star to collapse in on itself, creating a singularity - a point of infinite density and zero volume. The gravitational pull of the singularity is so strong that nothing, not even light, can escape its grasp, hence the name \"black hole\". \\n\\nThere are also supermassive black holes, which are found at the centers of galaxies and are thought to have formed through the merging of smaller black holes and the accretion of gas and dust. \\n\\nThe study of black holes is a fascinating and active area of research in astrophysics, and there is still much to be learned about these mysterious objects.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an AI research assistant. You use a tone that is technical and scientific.\"),\n",
    "    HumanMessage(content=\"Hello, who are you?\"),\n",
    "    AIMessage(content=\"Greeting! I am an AI research assistant. How can I help you today?\"),\n",
    "    HumanMessage(content=\"Can you tell me about the creation of black holes?\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using prompt templates for chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that can classify the sentiment of input texts. The labels you can use are {sentiment_labels}. Classify the following sentence:\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{user_input}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sentiment of the sentence \"I am doing brilliant today!\" is positive.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(chat_prompt.format_prompt(sentiment_labels=\"positive, negative, and neutral\", user_input=\"I am doing brilliant today!\").to_messages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The sentiment of the sentence \"Not sure what the weather is like today\" is neutral.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(chat_prompt.format_prompt(sentiment_labels=\"positive, negative, and neutral\", user_input=\"Not sure what the weather is like today.\").to_messages())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a template first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}?\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create a chain to prompt the model just using the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did the banana say to the elevator? \n",
      "A: \"Peel me a ride!\"\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"bananas\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining chains is particularly useful when you want to break tasks into subtasks for your applications. You can take the output of one chain to be the input to another chain. \n",
    "\n",
    "Example: We want to write a program that writes a joke then explains the joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first prompt\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}?\",\n",
    ")\n",
    "\n",
    "# second prompt\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"joke\"],\n",
    "    template=\"Explain the following joke: {joke}?\",\n",
    ")\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the chains using SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "Q: Why did the banana go to the doctor?\n",
      "A: Because it wasn't peeling well!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "This joke plays on the fact that when someone is feeling unwell, they typically go to see a doctor. The punchline of the joke is that the banana isn't feeling well because it isn't peeling well, which is not something that a doctor can help with.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "This joke plays on the fact that when someone is feeling unwell, they typically go to see a doctor. The punchline of the joke is that the banana isn't feeling well because it isn't peeling well, which is not something that a doctor can help with.\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "\n",
    "explanation = overall_chain.run(\"bananas\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
